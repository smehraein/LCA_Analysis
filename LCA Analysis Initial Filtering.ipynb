{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Filtering - Extract from .xlsx, filter, and write to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a massive amount of data in this .xlsx file (over 500,000 rows). It contains all LCA requests from the year 2014. My first goal is to extract it from Excel format, drop rows and columns that I won't need, and write it to a csv for easier access.\n",
    "\n",
    "I originally planned to use the past 3 years of data, but that was simply too much for my little 13' Mac to handle. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading\n",
      "Done Reading\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f = open(\"2014_LCA.xlsx\", \"r\")\n",
    "\n",
    "print \"Reading\"  # This takes a while...\n",
    "raw_df = pd.read_excel(f)\n",
    "print \"Done Reading\"\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the associated .doc file, I can determine which columns aren't going to be valuable for the analysis I have planned:\n",
    "\n",
    "\"VISA_CLASS\" - These are all H1B  \n",
    "\"PW_SOURCE_1\" - Wage source info  \n",
    "\"OTHER_WAGE_SOURCE_1\" - Wage source info  \n",
    "\"YR_SOURCE_PUB_1\" - Wage source info  \n",
    "\"PW_UNIT_2\"  \n",
    "\"PW_SOURCE_2\"  \n",
    "\"OTHER_WAGE_SOURCE_2\"  \n",
    "\"YR_SOURCE_PUB_2\"  \n",
    "\n",
    "In addition, let's check the number of null values in each column to see if there are any very sparse columns to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCA_CASE_NUMBER                        0\n",
      "STATUS                                 0\n",
      "LCA_CASE_SUBMIT                        0\n",
      "DECISION_DATE                          0\n",
      "VISA_CLASS                             0\n",
      "LCA_CASE_EMPLOYMENT_START_DATE         4\n",
      "LCA_CASE_EMPLOYMENT_END_DATE           5\n",
      "LCA_CASE_EMPLOYER_NAME                28\n",
      "LCA_CASE_EMPLOYER_ADDRESS             42\n",
      "LCA_CASE_EMPLOYER_CITY                27\n",
      "LCA_CASE_EMPLOYER_STATE               46\n",
      "LCA_CASE_EMPLOYER_POSTAL_CODE         41\n",
      "LCA_CASE_SOC_CODE                     49\n",
      "LCA_CASE_SOC_NAME                   4271\n",
      "LCA_CASE_JOB_TITLE                     7\n",
      "LCA_CASE_WAGE_RATE_FROM               36\n",
      "LCA_CASE_WAGE_RATE_TO             416572\n",
      "LCA_CASE_WAGE_RATE_UNIT               36\n",
      "FULL_TIME_POS                          5\n",
      "TOTAL_WORKERS                          3\n",
      "LCA_CASE_WORKLOC1_CITY                49\n",
      "LCA_CASE_WORKLOC1_STATE               49\n",
      "PW_1                                  84\n",
      "PW_UNIT_1                             77\n",
      "PW_SOURCE_1                           80\n",
      "OTHER_WAGE_SOURCE_1                11413\n",
      "YR_SOURCE_PUB_1                       84\n",
      "LCA_CASE_WORKLOC2_CITY            435616\n",
      "LCA_CASE_WORKLOC2_STATE           435598\n",
      "PW_2                              435595\n",
      "PW_UNIT_2                         435594\n",
      "PW_SOURCE_2                       435596\n",
      "OTHER_WAGE_SOURCE_2               436810\n",
      "YR_SOURCE_PUB_2                   436810\n",
      "LCA_CASE_NAICS_CODE                   25\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the second work place and WAGE_RATE_TO columns are too sparse to work with, so I'll drop those. The full list of columns to drop is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unwanted_columns = [\"VISA_CLASS\", \"LCA_CASE_WAGE_RATE_TO\", \"PW_SOURCE_1\", \"OTHER_WAGE_SOURCE_1\", \"YR_SOURCE_PUB_1\", \n",
    "                    \"LCA_CASE_WORKLOC2_CITY\", \"LCA_CASE_WORKLOC2_CITY\", \"LCA_CASE_WORKLOC2_STATE\", \n",
    "                    \"PW_2\", \"PW_UNIT_2\", \"PW_SOURCE_2\",\"OTHER_WAGE_SOURCE_2\", \"YR_SOURCE_PUB_2\"]\n",
    "\n",
    "raw_df = raw_df.drop(unwanted_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LCA_CASE_SOC_NAME column is missing plenty of items, but is directly linked to the SOC_CODE column preceding it, so I can probably fill in most of the missing values by comparing rows with the same SOC_CODE. Let's get a list of the codes for these missing names, then print out the values associated with those codes. As long as there's a non-null value associated with a code, I can fill in the other rows it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "soc_null_codes = raw_df.LCA_CASE_SOC_CODE[raw_df.LCA_CASE_SOC_NAME.isnull()].unique()\n",
    "\n",
    "code_name_list = []\n",
    "\n",
    "for code in soc_null_codes:\n",
    "    names = raw_df.LCA_CASE_SOC_NAME[raw_df.LCA_CASE_SOC_CODE == code].unique()\n",
    "    if len(names) > 1: # If there's something other than NaN here...\n",
    "        code_name_list.append([code, names])\n",
    "\n",
    "print code_name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, so much for that. I'll use dropna() to remove rows with any null values. The biggest hit will come from dropping these rows with no SOC_NAME, but we have 500,000+ data points and this represents less than 10% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_df = raw_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need to clean up the EMPLOYER_NAME column. I later found that many rows were being mis-grouped due to this (ex. \"GOOGLE, INC.\" and \"GOOGLE  INC.\" would count as separate employers). I'll try to go about doing this by first grouping the rows by EMPLOYER_ADDRESS and EMPLOYER_POSTAL_CODE, then standardizing the names in all of those groups (the assumption being that there is only one company per street address per zip code).\n",
    "\n",
    "However, I first need to clean up the POSTAL_CODE column. In most cases, the value is a standard 5 number ZIP code, but there are some full 9-digit ones that should be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCA_CASE_EMPLOYER_ADDRESS       LCA_CASE_EMPLOYER_POSTAL_CODE\n",
      "#1 CHILDREN'S WAY               72202                             1\n",
      "#1 HIGHLANDER WAY               33853                             1\n",
      "#1 SMOKY VALLEY MINE ROAD       89045                             2\n",
      "#16-529 WEST 48TH ST            10019                             1\n",
      "#2 JOHN BREWERS BAY             00802                             3\n",
      "#2 SAINT ANTHONY'S WAY          62002                             1\n",
      "#202, 5609 COLLEGE AVE          94618                             2\n",
      "#233, 75 BROADWAY               94111                            22\n",
      "#3 NATURAL RESOURCES DRIVE      72215                             3\n",
      "#3 RESEARCH PARK DRIVE          63304                             1\n",
      "#48 PR STATE ROAD 165 KM 1.2    00968                             1\n",
      "#5 STATEHOUSE PLAZA, SUITE 500  72201                             1\n",
      "#50 COUNTY ROAD 13              87013                             1\n",
      "#6 CITYPLACE DRIVE              63141                             2\n",
      "#6, ROCK ISLAND                 73401                             2\n",
      "...\n",
      "YALE UNIVERSITY            06520                              1\n",
      "YALE UNIVERSITY  / OISS    06520                              2\n",
      "YALE UNIVERSITY / OISS     06520                             19\n",
      "YALE UNIVERSITY OISS       06520                              7\n",
      "YALE UNIVERSITY, OISS      06520                              2\n",
      "YALE UNIVERSITY/  OISS     06520                              2\n",
      "YALE UNIVERSITY/ O ISS     06520                              1\n",
      "YALE UNIVERSITY/ OISS      06520                             66\n",
      "                           06525                              1\n",
      "                           0T652                              1\n",
      "YALE UNIVERSITY/OISS       05620                              1\n",
      "                           06510                              1\n",
      "                           06520                            107\n",
      "YMCA BUILDING  ROOM 101A   77843                              1\n",
      "ZACHARY TAYLOR CIRCLE      20171                              1\n",
      "Length: 65007, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "raw_df[\"LCA_CASE_EMPLOYER_POSTAL_CODE\"] = [str(t)[:5] for t in raw_df[\"LCA_CASE_EMPLOYER_POSTAL_CODE\"]]\n",
    "\n",
    "grouped_by_location = raw_df.groupby([\"LCA_CASE_EMPLOYER_ADDRESS\", \"LCA_CASE_EMPLOYER_POSTAL_CODE\"])\n",
    "\n",
    "print grouped_by_location.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, Yale seems to have proven that the street addresses and the ZIP codes are just as error prone as the company names (you can find a zip code with a letter in it, if you look hard enough). I'll go for the low-hanging fruit and deformat the EMPLOYER_NAME column for now. This should fix most of the issues with name mis-matches, but the key thing going forward will be to use functions such as .contains() rather than exact matches to perform searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_df[\"LCA_CASE_EMPLOYER_NAME\"] = raw_df[\"LCA_CASE_EMPLOYER_NAME\"].str.replace(\",\", \"\")  # remove \",\"\n",
    "raw_df[\"LCA_CASE_EMPLOYER_NAME\"] = raw_df[\"LCA_CASE_EMPLOYER_NAME\"].str.replace(\".\", \"\")  # remove \".\"\n",
    "raw_df[\"LCA_CASE_EMPLOYER_NAME\"] = raw_df[\"LCA_CASE_EMPLOYER_NAME\"].str.replace(\" - \", \"-\")  # remove whitespace\n",
    "raw_df[\"LCA_CASE_EMPLOYER_NAME\"] = raw_df[\"LCA_CASE_EMPLOYER_NAME\"].str.replace(\"  \", \" \")  # remove double spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the STATUS column. This is one of the most important columns, displaying the status of each LCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS\n",
      "CERTIFIED              451389\n",
      "CERTIFIED-WITHDRAWN     36008\n",
      "DENIED                  11768\n",
      "INVALIDATED                 1\n",
      "REJECTED                    2\n",
      "WITHDRAWN               15874\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "grouped_by_status = raw_df.groupby(\"STATUS\")\n",
    "\n",
    "print grouped_by_status.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure whether INVALIDATED means WITHDRAWN or DENIED, so I'll drop that row (it's only 1 row). REJECTED should clearly be DENIED, however.\n",
    "\n",
    "I originally wanted to drop all the WITHDRAWN rows because they don't provide information about whether or not an LCA was approved, but I ultimately decided to keep them to better understand which companies and industries are applying for LCAs in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_df = raw_df[raw_df.STATUS != \"INVALIDATED\"]\n",
    "raw_df = raw_df.replace(\"REJECTED\", \"DENIED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a last look at all the numeric columns and see if there are any values which are clearly wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       LCA_CASE_WAGE_RATE_FROM  TOTAL_WORKERS          PW_1  \\\n",
      "count             5.150410e+05  515041.000000  5.150410e+05   \n",
      "mean              9.243991e+04       1.956689  6.977173e+04   \n",
      "std               1.021826e+07       5.867164  1.542514e+06   \n",
      "min               7.010000e+00       1.000000  0.000000e+00   \n",
      "25%               6.000000e+04       1.000000  5.222900e+04   \n",
      "50%               7.000000e+04       1.000000  6.333600e+04   \n",
      "75%               9.000000e+04       1.000000  7.974720e+04   \n",
      "max               7.278873e+09    2013.000000  8.201323e+08   \n",
      "\n",
      "       LCA_CASE_NAICS_CODE  \n",
      "count         5.150410e+05  \n",
      "mean          4.725340e+05  \n",
      "std           1.610983e+06  \n",
      "min           1.100000e+01  \n",
      "25%           5.182100e+05  \n",
      "50%           5.415110e+05  \n",
      "75%           5.415110e+05  \n",
      "max           8.129908e+08  \n"
     ]
    }
   ],
   "source": [
    "print raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there are errors in the TOTAL_WORKERS and PW_1 columns. There is a row requesting 2013 total workers (a misplaced date, most likely) and a row claiming a wage of $0.00. I'll drop these and take another look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       LCA_CASE_WAGE_RATE_FROM  TOTAL_WORKERS          PW_1  \\\n",
      "count             5.150380e+05  515038.000000  5.150380e+05   \n",
      "mean              9.244013e+04       1.952788  6.977204e+04   \n",
      "std               1.021829e+07       5.154741  1.542519e+06   \n",
      "min               7.010000e+00       1.000000  7.010000e+00   \n",
      "25%               6.000000e+04       1.000000  5.222900e+04   \n",
      "50%               7.000000e+04       1.000000  6.333600e+04   \n",
      "75%               9.000000e+04       1.000000  7.974780e+04   \n",
      "max               7.278873e+09     200.000000  8.201323e+08   \n",
      "\n",
      "       LCA_CASE_NAICS_CODE  \n",
      "count         5.150380e+05  \n",
      "mean          4.725353e+05  \n",
      "std           1.610988e+06  \n",
      "min           1.100000e+01  \n",
      "25%           5.182100e+05  \n",
      "50%           5.415110e+05  \n",
      "75%           5.415110e+05  \n",
      "max           8.129908e+08  \n"
     ]
    }
   ],
   "source": [
    "raw_df = raw_df[raw_df.PW_1 != 0]\n",
    "raw_df = raw_df[raw_df.TOTAL_WORKERS != 2013]\n",
    "\n",
    "print raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PW_1 (Prevailing Wage Rate) column looks fine now, but there's still a value of 200 in TOTAL_WORKERS. Since the 75% of that column is still 1, it's clearly an outlier - let's take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ORACLE AMERICA INC' u'SYNTEL CONSULTING INC' u'SYNTEL INC'\n",
      " u'ITELLIGENCE INC' u'CFA INSTITUTE' u'SYNOPSYS INC'\n",
      " u'IGATE TECHNOLOGIES INC' u'AMAZON CORPORATE LLC'\n",
      " u'QUALCOMM TECHNOLOGIES INC' u'WIPRO LIMITED'\n",
      " u'TATA CONSULTANCY SERVICES LIMITED' u'APPLE INC' u'ACCENTURE LLP'\n",
      " u'AMIRIT TECHNOLOGIES INC' u'RVS LINE LLC' u'BILINGUALS INC'\n",
      " u'JUNIPER NETWORKS (US) INC' u'JUNIPER NETWORKS INC'\n",
      " u'PRICEWATERHOUSECOOPERS LLP' u'NIIT TECHNOLOGIES LIMITED'\n",
      " u'PRICEWATERHOUSECOOPERS ADVISORY LLC' u'DETROIT EDUCATION & RESEARCH'\n",
      " u'CAYLEY AEROSPACE INC' u'PROGRAMMER RESOURCES INTERNATIONAL INC'\n",
      " u'PRESENCE SAINT FRANCIS HOSPITAL' u'WILLIAM BEAUMONT HOSPITAL'\n",
      " u'PRESENCE SAINT JOSEPH HOSPITAL-CHICAGO'\n",
      " u'SPRINGLEAF GENERAL SERVICES CORPORATION']\n"
     ]
    }
   ],
   "source": [
    "print str(raw_df[\"LCA_CASE_EMPLOYER_NAME\"][raw_df.TOTAL_WORKERS > 50].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there's actually several companies requesting more than 50 workers at a time. Let's look at Oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique TOTAL_WORKERS values from Oracle: [   1.  100.   10.    4.    5.   20.]\n",
      "Total applications from Oracle: 1073\n",
      "Total applications from Oracle where TOTAL_WORKERS == 20: 25\n",
      "Total applications from Oracle where TOTAL_WORKERS == 100: 1\n"
     ]
    }
   ],
   "source": [
    "oracle = raw_df[\"LCA_CASE_EMPLOYER_NAME\"]==\"ORACLE AMERICA INC\"\n",
    "\n",
    "print \"Unique TOTAL_WORKERS values from Oracle:\", raw_df[\"TOTAL_WORKERS\"][oracle].unique()\n",
    "\n",
    "print \"Total applications from Oracle:\", raw_df[\"TOTAL_WORKERS\"][oracle].count()\n",
    "\n",
    "print \"Total applications from Oracle where TOTAL_WORKERS == 20:\",\\\n",
    "                raw_df[\"TOTAL_WORKERS\"][oracle][raw_df[\"TOTAL_WORKERS\"]==20].count()\n",
    "    \n",
    "print \"Total applications from Oracle where TOTAL_WORKERS == 100:\",\\\n",
    "                raw_df[\"TOTAL_WORKERS\"][oracle][raw_df[\"TOTAL_WORKERS\"]==100].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So TOTAL_WORKERS isn't an accurate metric, which is good to know. Going forward, I won't use this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_df = raw_df.drop(\"TOTAL_WORKERS\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I'm ready to write this dataframe to a csv. There are some non-ASCII characters here, so I'll need to set the encoding to utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_df.to_csv(\"Prepped_LCA.csv\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
